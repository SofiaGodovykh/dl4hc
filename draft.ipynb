{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a4bc215",
   "metadata": {},
   "source": [
    "### Data modeling imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4665616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Processing libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Machine learning libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import model_selection, svm, naive_bayes\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, vocab\n",
    "import torchtext\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# General-purpose libraries\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133617f",
   "metadata": {},
   "source": [
    "### Constants and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc62949",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/Users/sg/dev/dl4hc_proj/data/data_source\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4294d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ns = pd.read_csv(os.path.join(DATA_PATH, 'data_ns.csv'))\n",
    "data = pd.read_csv(os.path.join(DATA_PATH, 'data.csv'))\n",
    "encoder = LabelEncoder()\n",
    "vectorizer = TfidfVectorizer(max_features=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab7e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_svm(df, encoder, vectorizer):\n",
    "    for morbidity in df[\"class\"].unique():\n",
    "        data = df[df['class'] == morbidity]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data['data'], data['judgment'], test_size=0.20, shuffle=True)\n",
    "        Train_Y  = encoder.fit_transform(y_train)\n",
    "        Test_Y  = encoder.fit_transform(y_test)\n",
    "        Train_X_Tfidf = vectorizer.fit_transform(X_train)\n",
    "        Test_X_Tfidf = vectorizer.fit_transform(X_test)\n",
    "\n",
    "        SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "        SVM.fit(Train_X_Tfidf, y_train)\n",
    "        predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "        f1_macro = f1_score(y_test, predictions_SVM, average='macro')\n",
    "        f1_micro = f1_score(y_test, predictions_SVM, average='micro')\n",
    "\n",
    "        print(morbidity)\n",
    "        print(\"f1-macro\", f1_macro)\n",
    "        print(\"f1-micro\", f1_micro)\n",
    "    \n",
    "run_svm(data, encoder, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40ed744",
   "metadata": {},
   "source": [
    "### SVM performs poorly on some classes. The data is scewed. Training SVM with parameters from the paper and ExtraTreesClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d08e10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['judgment'], test_size=0.20, shuffle=True)\n",
    "Test_X_Tfidf = vectorizer.fit_transform(X_test)\n",
    "Train_X_Tfidf = vectorizer.fit_transform(X_train)\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf, y_train)\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "\n",
    "print(\"accuracy_score\", accuracy_score(predictions_SVM, y_test))\n",
    "f1_macro = f1_score(y_test, predictions_SVM, average='macro')\n",
    "f1_micro = f1_score(y_test, predictions_SVM, average='micro')\n",
    "print(\"f1-macro\", f1_macro)\n",
    "print(\"f1-micro\", f1_micro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e6ee1",
   "metadata": {},
   "source": [
    "### Trying out different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(verbose=True)\n",
    "feature_selection_model = ExtraTreesClassifier(n_estimators=100)\n",
    "transformer = SelectFromModel(feature_selection_model)\n",
    "Train_X_Tfidf_selected = transformer.fit_transform(Train_X_Tfidf, y_train)\n",
    "param_grid = {'kernel': ['linear', 'poly', 'rbf'], 'gamma': [0.1, 1, 10]}\n",
    "grid_search = GridSearchCV(SVM, param_grid, cv=5, verbose = 2)\n",
    "grid_search.fit(Train_X_Tfidf_selected, y_train)\n",
    "print(\"Best Hyperparameters: \", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997653aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svm_model = svm.SVC(kernel=grid_search.best_params_['kernel'], gamma=grid_search.best_params_['gamma'])\n",
    "best_svm_model.fit(Train_X_Tfidf_selected, y_train)\n",
    "print(\"accuracy_score\", accuracy_score(predictions_SVM, y_test))\n",
    "f1_macro = f1_score(y_test, predictions_SVM, average='macro')\n",
    "f1_micro = f1_score(y_test, predictions_SVM, average='micro')\n",
    "print(\"f1-macro\", f1_macro)\n",
    "print(\"f1-micro\", f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ddcc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = svm.SVC(verbose=True)\n",
    "feature_selection_model = sklearn.feature_selection.SelectKBest()\n",
    "transformer = SelectFromModel(feature_selection_model)\n",
    "Train_X_Tfidf_selected = transformer.fit_transform(Train_X_Tfidf, y_train)\n",
    "param_grid = {'kernel': ['linear'], 'gamma': [0.1]}\n",
    "grid_search = GridSearchCV(SVM, param_grid, cv=5, verbose = 2)\n",
    "grid_search.fit(Train_X_Tfidf_selected, y_train)\n",
    "print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
    "best_svm_model = svm.SVC(kernel=grid_search.best_params_['kernel'], gamma=grid_search.best_params_['gamma'])\n",
    "best_svm_model.fit(Train_X_Tfidf_selected, y_train)\n",
    "print(\"accuracy_score\", accuracy_score(predictions_SVM, y_test))\n",
    "f1_macro = f1_score(y_test, predictions_SVM, average='macro')\n",
    "f1_micro = f1_score(y_test, predictions_SVM, average='micro')\n",
    "print(\"f1-macro\", f1_macro)\n",
    "print(\"f1-micro\", f1_micro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f6967",
   "metadata": {},
   "source": [
    "### Calculating F-1 score and comparing other models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39395ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [svm.SVC(kernel='linear', gamma=0.1, verbose=True), RandomForestClassifier(verbose=True), sklearn.naive_bayes.GaussianNB()]\n",
    "models = [RandomForestClassifier(verbose=True), sklearn.naive_bayes.GaussianNB()]\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['judgment'], test_size=0.20, shuffle=True)\n",
    "X_test = vectorizer.fit_transform(X_test)\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "for model in models:\n",
    "    f1_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_micro')\n",
    "    avg_f1_score = np.mean(f1_scores)\n",
    "    print(f\"Model: {type(model).__name__}, Avg F-1 Score: {avg_f1_score}\")\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1_score_macro = f1_score(y_test, y_pred, average='micro')\n",
    "    print(f\"Model: {type(model).__name__}, F-1 Score on Test Data: {f1_score_macro}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e213413",
   "metadata": {},
   "source": [
    "### Creating and training DL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a2d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordsDataset(Dataset):\n",
    "    def __init__(self, dataframe, disease):\n",
    "        self.disease = disease\n",
    "        self.df = dataframe[dataframe['disease'] == disease].copy()\n",
    "        self.df = self.df.reset_index()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        Y = self.df.iloc[i]['judgment']\n",
    "        X = self.df.iloc[i]['text']\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61982830",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MorbidityLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MorbidityLSTM, self).__init__()\n",
    "        self.max_tokens = 1500\n",
    "        self.dropout = 0.1\n",
    "        self.hidden_size = 128\n",
    "        self.embedding_dim = 300\n",
    "        weights = np.zeros((len(vocab), self.embedding_dim))\n",
    "\n",
    "        for i in range(0,len(vocab)-1):\n",
    "            word = vocab.lookup_token(i)\n",
    "            weights[i] = glove_embeddings.get_vecs_by_tokens(word)\n",
    "\n",
    "        self.em = nn.Embedding.from_pretrained(torch.tensor(weights).float(), freeze=False)\n",
    "        self.hidden_dim1 = self.hidden_size\n",
    "        self.hidden_dim2 = int(self.hidden_size/2)\n",
    "        self.num_layers = 1\n",
    "        self.bilstm1 = nn.LSTM(input_size = self.embedding_dim, hidden_size = int(self.hidden_dim1/2), bidirectional = True,  \n",
    "                               batch_first = True, num_layers = self.num_layers) \n",
    "        self.bilstm2 = nn.LSTM(input_size = self.hidden_dim1, hidden_size = int(self.hidden_dim2/2), bidirectional = True,  \n",
    "                               batch_first = True, num_layers=self.num_layers)\n",
    "\n",
    "        self.do = nn.Dropout(self.dropout)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(self.hidden_dim2 * self.max_tokens, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.em(x)  \n",
    "        x, _ = self.bilstm1(x)\n",
    "        x, _ = self.bilstm2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.do(x)\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3376e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, n_epoch, lr):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "        epoch = epoch+1\n",
    "        curr_epoch_loss = []\n",
    "\n",
    "        for X, Y in tqdm(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = model(X.to(torch.device('cpu')))\n",
    "            loss = loss_func(y_hat, Y.to(device))  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
    "        loss_list += curr_epoch_loss\n",
    "        \n",
    "    return model, loss_list\n",
    "\n",
    "def _eval(model, dataloader):\n",
    "    model.eval()\n",
    "    pred_all = []\n",
    "    Y_test = []\n",
    "    for X, Y in tqdm(dataloader):\n",
    "        y_hat = emodel(X.to(torch.device('cpu')))\n",
    "        pred_all.append(y_hat.cpu().data.numpy())\n",
    "        Y_test.append(Y.cpu().data.numpy())\n",
    "    pred_all = np.concatenate(pred_all, axis=0)\n",
    "    Y_test = np.concatenate(Y_test, axis=0)\n",
    "\n",
    "    return pred_all, Y_test\n",
    "\n",
    "def _collate(batch):\n",
    "    batch_size = len(batch)\n",
    "    X = torch.zeros(batch_size, len(batch[0][0]), dtype=torch.long)\n",
    "    Y = torch.zeros((batch_size), dtype=torch.long)\n",
    "    for i in range(len(batch)):\n",
    "        x, y = batch[i]\n",
    "        vectors = vocab.lookup_indices(x)\n",
    "        X[i] = torch.tensor(vectors).long()\n",
    "        Y[i] = torch.tensor(float(y == True))\n",
    "        \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./dl4hc/data.csv\")\n",
    "vocab = torch.load(\"./dl4hc/glove_vocab.obj\")\n",
    "glove_embeddings = torchtext.vocab.GloVe(name='6B', dim=300)   \n",
    "lr = 0.01\n",
    "n_epoch = 25\n",
    "for _,disease in enumerate(list(data[\"disease\"].unique())):\n",
    "    model = MorbidityLSTM()\n",
    "    collate=_collate\n",
    "    ds = RecordsDataset(df, disease)\n",
    "    ds_train, ds_test = train_test_split(ds, test_size=0.20, shuffle=True)\n",
    "    train_loader = torch.utils.data.DataLoader(ds_train, batch_size = 32, collate_fn=_collate)\n",
    "    val_loader = torch.utils.data.DataLoader(ds_test, batch_size = 32, collate_fn=_collate)\n",
    "    model, loss_list = train(model, train_loader, n_epoch, lr)\n",
    "    pred, truth = _eval(model, val_loader)\n",
    "    auroc, f1, f1_macro, f1_micro = evaluate_predictions(truth, pred)\n",
    "    print(auroc, f1, f1_macro, f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736dc0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
